{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended contennt.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit each url and take a look at its source through Chrome DevTools. You'll need to identify the html tags, special class names etc. used for the html content you are expected to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are imported for you. If you prefer to use additional libraries feel free to uncomment them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from lxml import html\n",
    "from lxml.html import fromstring\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "import random\n",
    "import re\n",
    "import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#your code\n",
    "hmtl = requests.get(url).content\n",
    "soup = BeautifulSoup(hmtl,\"html.parser\")\n",
    "mysoup = soup.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HungVietNguyen',\n",
       " 'MartenSeemann',\n",
       " 'Ehco',\n",
       " 'MikePerham',\n",
       " \"BrunoD'Luka\",\n",
       " 'JunrouNishida',\n",
       " 'mborgerson',\n",
       " 'WillMcGugan',\n",
       " 'AndreySitnik',\n",
       " 'DominicGannaway',\n",
       " 'HadleyWickham',\n",
       " 'CharlesKerr',\n",
       " 'GunnarMorling',\n",
       " 'MarkBaker',\n",
       " 'Steven!Ragnarök',\n",
       " 'StephenCelis',\n",
       " 'ManuelKaufmann',\n",
       " 'ShahedNasser',\n",
       " 'HaThach',\n",
       " 'AlexGrönholm',\n",
       " 'AndrewGunnerson',\n",
       " 'HenrikRydgård',\n",
       " 'RichHarris',\n",
       " 'hewigovens',\n",
       " 'KenoFischer']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "List_names = []\n",
    "for names in mysoup.find_all(\"h1\",class_ = \"h3 lh-condensed\"):\n",
    "    clean_names = re.sub(r\"[\\n\\t\\s]*\", \"\",names.get_text())\n",
    "    List_names.append(clean_names)\n",
    "List_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Display the trending Python repositories in GitHub\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#your code\n",
    "hmtl = requests.get(url).content\n",
    "soup = BeautifulSoup(hmtl,\"html.parser\")\n",
    "mysoup = soup.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data-engineering-with-databricks',\n",
       " 'best-of-ml-python',\n",
       " 'streamlit',\n",
       " 'transformers',\n",
       " 'DALLE2-pytorch',\n",
       " 'public-apis',\n",
       " 'sherlock',\n",
       " 'salt',\n",
       " 'mirror-leech-telegram-bot',\n",
       " 'pre-commit-hooks',\n",
       " 'scikit-learn',\n",
       " 'fastapi',\n",
       " '30-Days-Of-Python',\n",
       " 'Auto_Bangumi',\n",
       " 'rich',\n",
       " 'copilot-docs',\n",
       " 'TechXueXi',\n",
       " 'AzurLaneAutoScript',\n",
       " 'ansible',\n",
       " 'openpilot',\n",
       " 'mmclassification',\n",
       " 'recommenders',\n",
       " 'detectron2',\n",
       " 'InvenTree',\n",
       " 'pandas']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "List_names = []\n",
    "for names2 in mysoup.find_all(\"span\",class_=\"text-normal\"):\n",
    "    for names in names2.next_siblings:\n",
    "        clean_names = re.sub(r\"[\\n\\t\\s]*\",\"\",names.get_text())\n",
    "        List_names.append(clean_names)\n",
    "List_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#your code\n",
    "hmtl = requests.get(url).content\n",
    "soup = BeautifulSoup(hmtl,\"html.parser\")\n",
    "mysoup = soup.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/20px-Cscr-featured.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/8/8c/Extended-protection-shackle.svg/20px-Extended-protection-shackle.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/87/Walt_Disney_1942_signature.svg/150px-Walt_Disney_1942_signature.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Trolley_Troubles_poster.jpg/170px-Trolley_Troubles_poster.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/4/4e/Steamboat-willie.jpg/170px-Steamboat-willie.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/5/57/Walt_Disney_1935.jpg/170px-Walt_Disney_1935.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg/220px-Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/15/Disney_drawing_goofy.jpg/170px-Disney_drawing_goofy.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/13/DisneySchiphol1951.jpg/220px-DisneySchiphol1951.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/WaltDisneyplansDisneylandDec1954.jpg/220px-WaltDisneyplansDisneylandDec1954.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Walt_disney_portrait_right.jpg/170px-Walt_disney_portrait_right.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Walt_Disney_Grave.JPG/170px-Walt_Disney_Grave.JPG',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Roy_O._Disney_with_Company_at_Press_Conference.jpg/170px-Roy_O._Disney_with_Company_at_Press_Conference.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Disney_Oscar_1953_%28cropped%29.jpg/170px-Disney_Oscar_1953_%28cropped%29.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Disney1968.jpg/170px-Disney1968.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Disneyland_Resort_logo.svg/135px-Disneyland_Resort_logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/d/da/Animation_disc.svg/20px-Animation_disc.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/6/69/P_vip.svg/19px-P_vip.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Magic_Kingdom_castle.jpg/15px-Magic_Kingdom_castle.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Video-x-generic.svg/19px-Video-x-generic.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Flag_of_Los_Angeles_County%2C_California.svg/21px-Flag_of_Los_Angeles_County%2C_California.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Blank_television_set.svg/21px-Blank_television_set.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/a/a4/Flag_of_the_United_States.svg/21px-Flag_of_the_United_States.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/14px-Commons-logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/16px-Wikiquote-logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Wikidata-logo.svg/21px-Wikidata-logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n",
       " '//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1',\n",
       " '/static/images/footer/wikimedia-button.png',\n",
       " '/static/images/footer/poweredby_mediawiki_88x31.png']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_list = []\n",
    "for src in mysoup.find_all(\"img\"):\n",
    "    src_list.append((src['src']))\n",
    "src_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#your code\n",
    "hmtl = requests.get(url).content\n",
    "soup = BeautifulSoup(hmtl,\"html.parser\")\n",
    "mysoup = soup.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/w/load.php?lang=en&modules=ext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cskins.vector.styles.legacy%7Cwikibase.client.init&only=styles&skin=vector',\n",
       " '/w/load.php?lang=en&modules=startup&only=scripts&raw=1&skin=vector',\n",
       " '/w/load.php?lang=en&modules=site.styles&only=styles&skin=vector',\n",
       " '//upload.wikimedia.org',\n",
       " '//en.m.wikipedia.org/wiki/Python',\n",
       " '/w/index.php?title=Python&action=edit',\n",
       " '/static/apple-touch/wikipedia.png',\n",
       " '/static/favicon/wikipedia.ico',\n",
       " '/w/opensearch_desc.php',\n",
       " '//en.wikipedia.org/w/api.php?action=rsd',\n",
       " 'https://creativecommons.org/licenses/by-sa/3.0/',\n",
       " 'https://en.wikipedia.org/wiki/Python',\n",
       " '//meta.wikimedia.org',\n",
       " '//login.wikimedia.org',\n",
       " '#mw-head',\n",
       " '#searchInput',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/9/99/Wiktionary-logo-en-v2.svg/40px-Wiktionary-logo-en-v2.svg.png',\n",
       " 'https://en.wiktionary.org/wiki/Python',\n",
       " 'https://en.wiktionary.org/wiki/python',\n",
       " '#Snakes',\n",
       " '#Computing',\n",
       " '#People',\n",
       " '#Roller_coasters',\n",
       " '#Vehicles',\n",
       " '#Weaponry',\n",
       " '#Other_uses',\n",
       " '#See_also',\n",
       " '/w/index.php?title=Python&action=edit&section=1',\n",
       " '/wiki/Pythonidae',\n",
       " '/wiki/Python_(genus)',\n",
       " '/wiki/Python_(mythology)',\n",
       " '/w/index.php?title=Python&action=edit&section=2',\n",
       " '/wiki/Python_(programming_language)',\n",
       " '/wiki/CMU_Common_Lisp',\n",
       " '/wiki/PERQ#PERQ_3',\n",
       " '/w/index.php?title=Python&action=edit&section=3',\n",
       " '/wiki/Python_of_Aenus',\n",
       " '/wiki/Python_(painter)',\n",
       " '/wiki/Python_of_Byzantium',\n",
       " '/wiki/Python_of_Catana',\n",
       " '/wiki/Python_Anghelo',\n",
       " '/w/index.php?title=Python&action=edit&section=4',\n",
       " '/wiki/Python_(Efteling)',\n",
       " '/wiki/Python_(Busch_Gardens_Tampa_Bay)',\n",
       " '/wiki/Python_(Coney_Island,_Cincinnati,_Ohio)',\n",
       " '/w/index.php?title=Python&action=edit&section=5',\n",
       " '/wiki/Python_(automobile_maker)',\n",
       " '/wiki/Python_(Ford_prototype)',\n",
       " '/w/index.php?title=Python&action=edit&section=6',\n",
       " '/wiki/Python_(missile)',\n",
       " '/wiki/Python_(nuclear_primary)',\n",
       " '/wiki/Colt_Python',\n",
       " '/w/index.php?title=Python&action=edit&section=7',\n",
       " '/wiki/PYTHON',\n",
       " '/wiki/Python_(film)',\n",
       " '/wiki/Monty_Python',\n",
       " '/wiki/Python_(Monty)_Pictures',\n",
       " '/wiki/Timon_of_Phlius',\n",
       " '/w/index.php?title=Python&action=edit&section=8',\n",
       " '/wiki/Cython',\n",
       " '/wiki/Pyton',\n",
       " '/wiki/Pithon',\n",
       " '/wiki/File:Disambig_gray.svg',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/5/5f/Disambig_gray.svg/30px-Disambig_gray.svg.png',\n",
       " '/wiki/Help:Disambiguation',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Special:WhatLinksHere/Python&namespace=0',\n",
       " '//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Python&oldid=1087251762',\n",
       " '/wiki/Help:Category',\n",
       " '/wiki/Category:Disambiguation_pages',\n",
       " '/wiki/Category:Human_name_disambiguation_pages',\n",
       " '/wiki/Category:Disambiguation_pages_with_given-name-holder_lists',\n",
       " '/wiki/Category:Disambiguation_pages_with_short_descriptions',\n",
       " '/wiki/Category:Short_description_is_different_from_Wikidata',\n",
       " '/wiki/Category:All_article_disambiguation_pages',\n",
       " '/wiki/Category:All_disambiguation_pages',\n",
       " '/wiki/Category:Animal_common_name_disambiguation_pages',\n",
       " '/wiki/Special:MyTalk',\n",
       " '/wiki/Special:MyContributions',\n",
       " '/w/index.php?title=Special:CreateAccount&returnto=Python',\n",
       " '/w/index.php?title=Special:UserLogin&returnto=Python',\n",
       " '/wiki/Python',\n",
       " '/wiki/Talk:Python',\n",
       " '/wiki/Python',\n",
       " '/w/index.php?title=Python&action=edit',\n",
       " '/w/index.php?title=Python&action=history',\n",
       " '/wiki/Main_Page',\n",
       " '/wiki/Main_Page',\n",
       " '/wiki/Wikipedia:Contents',\n",
       " '/wiki/Portal:Current_events',\n",
       " '/wiki/Special:Random',\n",
       " '/wiki/Wikipedia:About',\n",
       " '//en.wikipedia.org/wiki/Wikipedia:Contact_us',\n",
       " 'https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en',\n",
       " '/wiki/Help:Contents',\n",
       " '/wiki/Help:Introduction',\n",
       " '/wiki/Wikipedia:Community_portal',\n",
       " '/wiki/Special:RecentChanges',\n",
       " '/wiki/Wikipedia:File_Upload_Wizard',\n",
       " '/wiki/Special:WhatLinksHere/Python',\n",
       " '/wiki/Special:RecentChangesLinked/Python',\n",
       " '/wiki/Wikipedia:File_Upload_Wizard',\n",
       " '/wiki/Special:SpecialPages',\n",
       " '/w/index.php?title=Python&oldid=1087251762',\n",
       " '/w/index.php?title=Python&action=info',\n",
       " '/w/index.php?title=Special:CiteThisPage&page=Python&id=1087251762&wpFormIdentifier=titleform',\n",
       " 'https://www.wikidata.org/wiki/Special:EntityPage/Q747452',\n",
       " '/w/index.php?title=Special:DownloadAsPdf&page=Python&action=show-download-screen',\n",
       " '/w/index.php?title=Python&printable=yes',\n",
       " 'https://commons.wikimedia.org/wiki/Category:Python',\n",
       " 'https://af.wikipedia.org/wiki/Python',\n",
       " 'https://als.wikipedia.org/wiki/Python',\n",
       " 'https://ar.wikipedia.org/wiki/%D8%A8%D8%A7%D9%8A%D8%AB%D9%88%D9%86_(%D8%AA%D9%88%D8%B6%D9%8A%D8%AD)',\n",
       " 'https://az.wikipedia.org/wiki/Python',\n",
       " 'https://bn.wikipedia.org/wiki/%E0%A6%AA%E0%A6%BE%E0%A6%87%E0%A6%A5%E0%A6%A8_(%E0%A6%A6%E0%A7%8D%E0%A6%AC%E0%A7%8D%E0%A6%AF%E0%A6%B0%E0%A7%8D%E0%A6%A5%E0%A6%A4%E0%A6%BE_%E0%A6%A8%E0%A6%BF%E0%A6%B0%E0%A6%B8%E0%A6%A8)',\n",
       " 'https://be.wikipedia.org/wiki/Python',\n",
       " 'https://bg.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%BF%D0%BE%D1%8F%D1%81%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5)',\n",
       " 'https://cs.wikipedia.org/wiki/Python_(rozcestn%C3%ADk)',\n",
       " 'https://da.wikipedia.org/wiki/Python',\n",
       " 'https://de.wikipedia.org/wiki/Python',\n",
       " 'https://eo.wikipedia.org/wiki/Pitono_(apartigilo)',\n",
       " 'https://eu.wikipedia.org/wiki/Python_(argipena)',\n",
       " 'https://fa.wikipedia.org/wiki/%D9%BE%D8%A7%DB%8C%D8%AA%D9%88%D9%86',\n",
       " 'https://fr.wikipedia.org/wiki/Python',\n",
       " 'https://ko.wikipedia.org/wiki/%ED%8C%8C%EC%9D%B4%EC%84%A0',\n",
       " 'https://hr.wikipedia.org/wiki/Python_(razdvojba)',\n",
       " 'https://io.wikipedia.org/wiki/Pitono',\n",
       " 'https://id.wikipedia.org/wiki/Python',\n",
       " 'https://ia.wikipedia.org/wiki/Python_(disambiguation)',\n",
       " 'https://is.wikipedia.org/wiki/Python_(a%C3%B0greining)',\n",
       " 'https://it.wikipedia.org/wiki/Python_(disambigua)',\n",
       " 'https://he.wikipedia.org/wiki/%D7%A4%D7%99%D7%AA%D7%95%D7%9F',\n",
       " 'https://ka.wikipedia.org/wiki/%E1%83%9E%E1%83%98%E1%83%97%E1%83%9D%E1%83%9C%E1%83%98_(%E1%83%9B%E1%83%A0%E1%83%90%E1%83%95%E1%83%90%E1%83%9A%E1%83%9B%E1%83%9C%E1%83%98%E1%83%A8%E1%83%95%E1%83%9C%E1%83%94%E1%83%9A%E1%83%9D%E1%83%95%E1%83%90%E1%83%9C%E1%83%98)',\n",
       " 'https://kg.wikipedia.org/wiki/Mboma_(nyoka)',\n",
       " 'https://la.wikipedia.org/wiki/Python_(discretiva)',\n",
       " 'https://lb.wikipedia.org/wiki/Python',\n",
       " 'https://hu.wikipedia.org/wiki/Python_(egy%C3%A9rtelm%C5%B1s%C3%ADt%C5%91_lap)',\n",
       " 'https://mr.wikipedia.org/wiki/%E0%A4%AA%E0%A4%BE%E0%A4%AF%E0%A4%A5%E0%A5%89%E0%A4%A8_(%E0%A4%86%E0%A4%9C%E0%A5%8D%E0%A4%9E%E0%A4%BE%E0%A4%B5%E0%A4%B2%E0%A5%80_%E0%A4%AD%E0%A4%BE%E0%A4%B7%E0%A4%BE)',\n",
       " 'https://nl.wikipedia.org/wiki/Python',\n",
       " 'https://ja.wikipedia.org/wiki/%E3%83%91%E3%82%A4%E3%82%BD%E3%83%B3',\n",
       " 'https://no.wikipedia.org/wiki/Pyton',\n",
       " 'https://pl.wikipedia.org/wiki/Pyton',\n",
       " 'https://pt.wikipedia.org/wiki/Python_(desambigua%C3%A7%C3%A3o)',\n",
       " 'https://ru.wikipedia.org/wiki/Python_(%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D1%8F)',\n",
       " 'https://sk.wikipedia.org/wiki/Python',\n",
       " 'https://sr.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%B2%D0%B8%D1%88%D0%B5%D0%B7%D0%BD%D0%B0%D1%87%D0%BD%D0%B0_%D0%BE%D0%B4%D1%80%D0%B5%D0%B4%D0%BD%D0%B8%D1%86%D0%B0)',\n",
       " 'https://sh.wikipedia.org/wiki/Python',\n",
       " 'https://fi.wikipedia.org/wiki/Python',\n",
       " 'https://sv.wikipedia.org/wiki/Pyton',\n",
       " 'https://th.wikipedia.org/wiki/%E0%B9%84%E0%B8%9E%E0%B8%97%E0%B8%AD%E0%B8%99',\n",
       " 'https://tr.wikipedia.org/wiki/Python_(anlam_ayr%C4%B1m%C4%B1)',\n",
       " 'https://uk.wikipedia.org/wiki/%D0%9F%D1%96%D1%84%D0%BE%D0%BD',\n",
       " 'https://ur.wikipedia.org/wiki/%D9%BE%D8%A7%D8%A6%DB%8C%D8%AA%DA%BE%D9%88%D9%86',\n",
       " 'https://vi.wikipedia.org/wiki/Python',\n",
       " 'https://zh.wikipedia.org/wiki/Python_(%E6%B6%88%E6%AD%A7%E4%B9%89)',\n",
       " 'https://www.wikidata.org/wiki/Special:EntityPage/Q747452#sitelinks-wikipedia',\n",
       " '//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License',\n",
       " '//creativecommons.org/licenses/by-sa/3.0/',\n",
       " '//foundation.wikimedia.org/wiki/Terms_of_Use',\n",
       " '//foundation.wikimedia.org/wiki/Privacy_policy',\n",
       " '//www.wikimediafoundation.org/',\n",
       " 'https://foundation.wikimedia.org/wiki/Privacy_policy',\n",
       " '/wiki/Wikipedia:About',\n",
       " '/wiki/Wikipedia:General_disclaimer',\n",
       " '//en.wikipedia.org/wiki/Wikipedia:Contact_us',\n",
       " '//en.m.wikipedia.org/w/index.php?title=Python&mobileaction=toggle_view_mobile',\n",
       " 'https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute',\n",
       " 'https://stats.wikimedia.org/#/en.wikipedia.org',\n",
       " 'https://foundation.wikimedia.org/wiki/Cookie_statement',\n",
       " 'https://wikimediafoundation.org/',\n",
       " '/static/images/footer/wikimedia-button.png',\n",
       " 'https://www.mediawiki.org/',\n",
       " '/static/images/footer/poweredby_mediawiki_88x31.png']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "List_of_links = []\n",
    "for link in mysoup.find_all():\n",
    "    if 'href'in link.attrs:\n",
    "        List_of_links.append(link.attrs['href'])\n",
    "    if 'src'in link.attrs:\n",
    "        List_of_links.append(link.attrs['src'])\n",
    "List_of_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Titles that have changed in the United States Code since its last release point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#your code\n",
    "hmtl = requests.get(url).content\n",
    "soup = BeautifulSoup(hmtl,\"html.parser\")\n",
    "mysoup = soup.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "List= []\n",
    "for titles in mysoup.find_all('div',class_= 'usctitlechanged'):\n",
    "    clean_titles = re.sub(r\"[\\n\\t\\s]*\", \"\",titles.get_text())\n",
    "    List.append(clean_titles)\n",
    "len(List)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Python list with the top ten FBI's Most Wanted names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.fbi.gov/wanted/topten'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#your code hmtl = requests.get(url).content\n",
    "hmtl = requests.get(url).content\n",
    "soup = BeautifulSoup(hmtl,\"html.parser\")\n",
    "mysoup = soup.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BHADRESHKUMARCHETANBHAIPATEL',\n",
       " 'ALEJANDROROSALESCASTILLO',\n",
       " 'ARNOLDOJIMENEZ',\n",
       " 'JASONDEREKBROWN',\n",
       " 'ALEXISFLORES',\n",
       " 'JOSERODOLFOVILLARREAL-HERNANDEZ',\n",
       " 'RAFAELCARO-QUINTERO',\n",
       " 'YULANADONAYARCHAGACARIAS',\n",
       " 'EUGENEPALMER',\n",
       " 'OCTAVIANOJUAREZ-CORRO']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fbi_names= []\n",
    "for names in mysoup.find_all('h3',class_= 'title'):\n",
    "    clean_names = re.sub(r\"[\\n\\t\\s]*\", \"\",names.get_text())\n",
    "    Fbi_names.append(clean_names)\n",
    "Fbi_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#your code\n",
    "hmtl = requests.get(url).content\n",
    "soup = BeautifulSoup(hmtl,\"html.parser\")\n",
    "mysoup = soup.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latitude degrees</th>\n",
       "      <th>Latitude degrees</th>\n",
       "      <th>Longitude degrees</th>\n",
       "      <th>Longitude degrees</th>\n",
       "      <th>Region name [+]</th>\n",
       "      <th>Date &amp; Time UTC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.16</td>\n",
       "      <td>S</td>\n",
       "      <td>107.92</td>\n",
       "      <td>E</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2022-05-27 00:38:49.011min ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17.93</td>\n",
       "      <td>N</td>\n",
       "      <td>66.85</td>\n",
       "      <td>W</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2022-05-27 00:21:55.028min ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38.72</td>\n",
       "      <td>N</td>\n",
       "      <td>0.23</td>\n",
       "      <td>W</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2022-05-27 00:18:38.831min ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.72</td>\n",
       "      <td>N</td>\n",
       "      <td>99.79</td>\n",
       "      <td>W</td>\n",
       "      <td>4.3</td>\n",
       "      <td>2022-05-27 00:12:58.037min ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.98</td>\n",
       "      <td>N</td>\n",
       "      <td>66.85</td>\n",
       "      <td>W</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2022-05-27 00:08:43.541min ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16.60</td>\n",
       "      <td>N</td>\n",
       "      <td>99.85</td>\n",
       "      <td>W</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2022-05-27 00:06:40.043min ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>36.81</td>\n",
       "      <td>S</td>\n",
       "      <td>177.60</td>\n",
       "      <td>E</td>\n",
       "      <td>3.7</td>\n",
       "      <td>2022-05-26 23:56:52.853min ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.24</td>\n",
       "      <td>N</td>\n",
       "      <td>126.94</td>\n",
       "      <td>E</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2022-05-26 23:53:54.056min ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>35.44</td>\n",
       "      <td>N</td>\n",
       "      <td>3.60</td>\n",
       "      <td>W</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2022-05-26 23:44:25.51hr 05min ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>54.50</td>\n",
       "      <td>N</td>\n",
       "      <td>163.29</td>\n",
       "      <td>W</td>\n",
       "      <td>4.2</td>\n",
       "      <td>2022-05-26 23:36:04.01hr 14min ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7.71</td>\n",
       "      <td>S</td>\n",
       "      <td>118.76</td>\n",
       "      <td>E</td>\n",
       "      <td>3.3</td>\n",
       "      <td>2022-05-26 23:35:36.01hr 14min ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>59.67</td>\n",
       "      <td>N</td>\n",
       "      <td>153.31</td>\n",
       "      <td>W</td>\n",
       "      <td>3.3</td>\n",
       "      <td>2022-05-26 23:27:28.61hr 22min ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>23.55</td>\n",
       "      <td>S</td>\n",
       "      <td>67.35</td>\n",
       "      <td>W</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2022-05-26 23:24:59.01hr 25min ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18.97</td>\n",
       "      <td>N</td>\n",
       "      <td>68.68</td>\n",
       "      <td>W</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2022-05-26 23:21:09.31hr 28min ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.40</td>\n",
       "      <td>S</td>\n",
       "      <td>127.15</td>\n",
       "      <td>E</td>\n",
       "      <td>3.1</td>\n",
       "      <td>2022-05-26 23:20:55.01hr 29min ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>13.92</td>\n",
       "      <td>N</td>\n",
       "      <td>86.85</td>\n",
       "      <td>W</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2022-05-26 23:20:03.01hr 30min ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>40.36</td>\n",
       "      <td>N</td>\n",
       "      <td>19.55</td>\n",
       "      <td>E</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2022-05-26 23:19:13.41hr 30min ago</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Latitude degrees Latitude degrees Longitude degrees Longitude degrees  \\\n",
       "0              8.16                S            107.92                 E   \n",
       "1             17.93                N             66.85                 W   \n",
       "2             38.72                N              0.23                 W   \n",
       "3             16.72                N             99.79                 W   \n",
       "4             17.98                N             66.85                 W   \n",
       "5             16.60                N             99.85                 W   \n",
       "6               NaN              NaN               NaN               NaN   \n",
       "7               NaN              NaN               NaN               NaN   \n",
       "8               NaN              NaN               NaN               NaN   \n",
       "9             36.81                S            177.60                 E   \n",
       "10            10.24                N            126.94                 E   \n",
       "11            35.44                N              3.60                 W   \n",
       "12            54.50                N            163.29                 W   \n",
       "13             7.71                S            118.76                 E   \n",
       "14            59.67                N            153.31                 W   \n",
       "15            23.55                S             67.35                 W   \n",
       "16            18.97                N             68.68                 W   \n",
       "17             0.40                S            127.15                 E   \n",
       "18            13.92                N             86.85                 W   \n",
       "19            40.36                N             19.55                 E   \n",
       "\n",
       "   Region name [+]                     Date & Time UTC  \n",
       "0              4.0      2022-05-27 00:38:49.011min ago  \n",
       "1              2.5      2022-05-27 00:21:55.028min ago  \n",
       "2              1.6      2022-05-27 00:18:38.831min ago  \n",
       "3              4.3      2022-05-27 00:12:58.037min ago  \n",
       "4              2.1      2022-05-27 00:08:43.541min ago  \n",
       "5              4.0      2022-05-27 00:06:40.043min ago  \n",
       "6              NaN                                 NaN  \n",
       "7              NaN                                 NaN  \n",
       "8              NaN                                 NaN  \n",
       "9              3.7      2022-05-26 23:56:52.853min ago  \n",
       "10             3.4      2022-05-26 23:53:54.056min ago  \n",
       "11             1.9  2022-05-26 23:44:25.51hr 05min ago  \n",
       "12             4.2  2022-05-26 23:36:04.01hr 14min ago  \n",
       "13             3.3  2022-05-26 23:35:36.01hr 14min ago  \n",
       "14             3.3  2022-05-26 23:27:28.61hr 22min ago  \n",
       "15             2.7  2022-05-26 23:24:59.01hr 25min ago  \n",
       "16             3.5  2022-05-26 23:21:09.31hr 28min ago  \n",
       "17             3.1  2022-05-26 23:20:55.01hr 29min ago  \n",
       "18             2.8  2022-05-26 23:20:03.01hr 30min ago  \n",
       "19             2.2  2022-05-26 23:19:13.41hr 30min ago  "
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_html(url)\n",
    "df = df[3]\n",
    "df_new = df[{'Date & Time UTC','Latitude degrees','Longitude degrees','Region name [+]'}]\n",
    "df_new.columns = df_new.columns.get_level_values(0)\n",
    "df_new.head(20) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the date, days, title, city, country of next 25 hackathon events as a Pandas dataframe table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://mlh.io/seasons/2022/events'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hmtl = requests.get(url).content\n",
    "soup = BeautifulSoup(hmtl,\"html.parser\")\n",
    "mysoup = soup.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date and Days</th>\n",
       "      <th>Country</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Title</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LateNightHacks</th>\n",
       "      <td>May 27th - 29th</td>\n",
       "      <td>Worldwide</td>\n",
       "      <td>Everywhere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOHacks</th>\n",
       "      <td>May 28th - 29th</td>\n",
       "      <td>Worldwide</td>\n",
       "      <td>Everywhere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Women Techies 2022</th>\n",
       "      <td>May 28th - 29th</td>\n",
       "      <td>Worldwide</td>\n",
       "      <td>Everywhere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bon Voyage Hacks II</th>\n",
       "      <td>Jun 3rd - 5th</td>\n",
       "      <td>Worldwide</td>\n",
       "      <td>Everywhere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Coastal Hacks</th>\n",
       "      <td>Jun 10th - 12th</td>\n",
       "      <td>Worldwide</td>\n",
       "      <td>Everywhere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OneHacks II</th>\n",
       "      <td>Jun 10th - 12th</td>\n",
       "      <td>Worldwide</td>\n",
       "      <td>Everywhere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mental Health Hacks II</th>\n",
       "      <td>Jun 17th - 19th</td>\n",
       "      <td>Worldwide</td>\n",
       "      <td>Everywhere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WaffleHacks</th>\n",
       "      <td>Jun 17th - 19th</td>\n",
       "      <td>Worldwide</td>\n",
       "      <td>Everywhere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FreyHacks</th>\n",
       "      <td>Jun 24th - 26th</td>\n",
       "      <td>Worldwide</td>\n",
       "      <td>Everywhere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hacking Heist 2.0</th>\n",
       "      <td>Jun 24th - 26th</td>\n",
       "      <td>Worldwide</td>\n",
       "      <td>Everywhere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PrideHacks II</th>\n",
       "      <td>Jun 24th - 26th</td>\n",
       "      <td>Worldwide</td>\n",
       "      <td>Everywhere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SimpliHacks 2.0</th>\n",
       "      <td>Jun 24th - 26th</td>\n",
       "      <td>Worldwide</td>\n",
       "      <td>Everywhere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hack3</th>\n",
       "      <td>Jun 25th - 26th</td>\n",
       "      <td>Worldwide</td>\n",
       "      <td>Everywhere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GHW: INIT</th>\n",
       "      <td>Jul 3rd - 10th</td>\n",
       "      <td>Worldwide</td>\n",
       "      <td>Everywhere</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Date and Days    Country        City\n",
       "Title                                                                 \n",
       "LateNightHacks          May 27th - 29th          Worldwide  Everywhere\n",
       "TOHacks                 May 28th - 29th          Worldwide  Everywhere\n",
       "Women Techies 2022      May 28th - 29th          Worldwide  Everywhere\n",
       "Bon Voyage Hacks II       Jun 3rd - 5th          Worldwide  Everywhere\n",
       "Coastal Hacks           Jun 10th - 12th          Worldwide  Everywhere\n",
       "OneHacks II             Jun 10th - 12th          Worldwide  Everywhere\n",
       "Mental Health Hacks II  Jun 17th - 19th          Worldwide  Everywhere\n",
       "WaffleHacks             Jun 17th - 19th          Worldwide  Everywhere\n",
       "FreyHacks               Jun 24th - 26th          Worldwide  Everywhere\n",
       "Hacking Heist 2.0       Jun 24th - 26th          Worldwide  Everywhere\n",
       "PrideHacks II           Jun 24th - 26th          Worldwide  Everywhere\n",
       " SimpliHacks 2.0        Jun 24th - 26th          Worldwide  Everywhere\n",
       "Hack3                   Jun 25th - 26th          Worldwide  Everywhere\n",
       "GHW: INIT                Jul 3rd - 10th          Worldwide  Everywhere"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date = []\n",
    "title = []\n",
    "city = []\n",
    "country = []\n",
    "for names in mysoup.find_all('h3',class_='event-name'):\n",
    "    title.append(names.get_text())\n",
    "for dates in mysoup.find_all('p', class_= 'event-date'):\n",
    "    date.append(dates.get_text())\n",
    "for state in mysoup.find_all('span', itemprop= 'state'):\n",
    "    country.append(state.get_text())\n",
    "for citie in mysoup.find_all('span', itemprop= 'city'):\n",
    "    city.append(citie.get_text())\n",
    "columns = ['Title','Date and Days','Country','City']\n",
    "events = pd.DataFrame(columns = columns)\n",
    "events['Title'] = title\n",
    "events['Date and Days'] = date\n",
    "events['Country'] = country\n",
    "events['City'] = city\n",
    "events = events.set_index('Title')\n",
    "events = events[:14]\n",
    "events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#your code\n",
    "hmtl = requests.get(url).content\n",
    "soup = BeautifulSoup(hmtl,\"html.parser\")\n",
    "mysoup = soup.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = []\n",
    "articles = []\n",
    "for i in mysoup.find_all('strong'):\n",
    "    languages.append(i.get_text())\n",
    "for i in mysoup.find_all('small'):\n",
    "    i = i.get_text()\n",
    "    clean_data = i.replace(u'\\xa0','')\n",
    "    articles.append(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'English': '6458000+ articles',\n",
       " '日本語': '1314000+ 記事',\n",
       " 'Русский': '1798000+ статей',\n",
       " 'Español': '1755000+ artículos',\n",
       " 'Deutsch': '2667000+ Artikel',\n",
       " 'Français': '2400000+ articles',\n",
       " 'Italiano': '1742000+ voci',\n",
       " '中文': '1256000+ 条目 / 條目',\n",
       " 'Português': '1085000+ artigos',\n",
       " 'العربية': '1159000+ مقالة'}"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hice pop de las listas para quitar los que no queria\n",
    "dictionary = dict(zip(languages, articles))\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#your code \n",
    "hmtl = requests.get(url).content\n",
    "soup = BeautifulSoup(hmtl,\"html.parser\")\n",
    "mysoup = soup.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Business and economy',\n",
       " 'Crime and justice',\n",
       " 'Defence',\n",
       " 'Education',\n",
       " 'Environment',\n",
       " 'Government',\n",
       " 'Government spending',\n",
       " 'Health',\n",
       " 'Mapping',\n",
       " 'Society',\n",
       " 'Towns and cities',\n",
       " 'Transport',\n",
       " 'Digital service performance',\n",
       " 'Government reference data']"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list = []\n",
    "for i in mysoup.find_all('h3',class_ = \"govuk-heading-s dgu-topics__heading\"):\n",
    "    list.append(i.get_text())\n",
    "list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 languages by number of native speakers stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#your code\n",
    "hmtl = requests.get(url).content\n",
    "soup = BeautifulSoup(hmtl,\"html.parser\")\n",
    "mysoup = soup.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Native Speakers(millions)</th>\n",
       "      <th>Percentageof world pop.(March 2019)[10]</th>\n",
       "      <th>Language family</th>\n",
       "      <th>Branch</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mandarin Chinese</td>\n",
       "      <td>929.0</td>\n",
       "      <td>11.922%</td>\n",
       "      <td>Sino-Tibetan</td>\n",
       "      <td>Sinitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>474.7</td>\n",
       "      <td>5.994%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>English</td>\n",
       "      <td>372.9</td>\n",
       "      <td>4.922%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Germanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hindi (sanskritised Hindustani)[11]</td>\n",
       "      <td>343.9</td>\n",
       "      <td>4.429%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bengali</td>\n",
       "      <td>233.7</td>\n",
       "      <td>4.000%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>232.4</td>\n",
       "      <td>2.870%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Russian</td>\n",
       "      <td>154.0</td>\n",
       "      <td>2.000%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Balto-Slavic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>125.3</td>\n",
       "      <td>1.662%</td>\n",
       "      <td>Japonic</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Western Punjabi[12]</td>\n",
       "      <td>92.7</td>\n",
       "      <td>1.204%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Yue Chinese</td>\n",
       "      <td>85.2</td>\n",
       "      <td>0.949%</td>\n",
       "      <td>Sino-Tibetan</td>\n",
       "      <td>Sinitic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Language Native Speakers(millions)  \\\n",
       "Rank                                                                  \n",
       "1                        Mandarin Chinese                     929.0   \n",
       "2                                 Spanish                     474.7   \n",
       "3                                 English                     372.9   \n",
       "4     Hindi (sanskritised Hindustani)[11]                     343.9   \n",
       "5                                 Bengali                     233.7   \n",
       "6                              Portuguese                     232.4   \n",
       "7                                 Russian                     154.0   \n",
       "8                                Japanese                     125.3   \n",
       "9                     Western Punjabi[12]                      92.7   \n",
       "10                            Yue Chinese                      85.2   \n",
       "\n",
       "     Percentageof world pop.(March 2019)[10] Language family        Branch  \n",
       "Rank                                                                        \n",
       "1                                    11.922%    Sino-Tibetan       Sinitic  \n",
       "2                                     5.994%   Indo-European       Romance  \n",
       "3                                     4.922%   Indo-European      Germanic  \n",
       "4                                     4.429%   Indo-European    Indo-Aryan  \n",
       "5                                     4.000%   Indo-European    Indo-Aryan  \n",
       "6                                     2.870%   Indo-European       Romance  \n",
       "7                                     2.000%   Indo-European  Balto-Slavic  \n",
       "8                                     1.662%         Japonic      Japanese  \n",
       "9                                     1.204%   Indo-European    Indo-Aryan  \n",
       "10                                    0.949%    Sino-Tibetan       Sinitic  "
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_html(url)\n",
    "n_df = df[0]\n",
    "n_df = n_df[:10]\n",
    "n_df.set_index('Rank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = city=input('Enter the city:')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Book name,price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
